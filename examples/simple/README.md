# Easy example of spinnaker on aws

## Usage
You can use this module like as below. This shows how to create the resources for spinnaker.
This module will create vpc, subnets, s3 bucket, iam policies and kubernetes cluster.

### Initialise
This is the first step to create a spinnaker cluster. Just get terraform module and apply it and this is sort of terraform command to do `terraform init`, `terraform plan`, and `terraform apply`. After then you will see so many resources like eks, s3, ec2, iam, rds, and others on aws.

```
module "spinnaker" {
  source  = "tf-mod/spinnaker/aws"
  version = "1.0.0"

  name                 = "spinnaker"
  stack                = "preprod"
  region               = "us-east-1"
  azs                  = "["us-east-1a","us-east-1b","us-east-1c"]
  cidr                 = "10.51.0.0/16"
  ssl_cert_arn         = "arn:aws:acm:us-east-1:1234567890321:certificate/4a6b2a09-246a-4e7b-9850-a4251e123"
  kube_version         = "1.12"
  tags                 = "${map("env", "preprod")}"
}
```

### Update the trusted relationship
After you've done previous step to lanuch a spinnaker you can now start to integrate with other aws accounts as `spinnaker managed` to make them to be managed by your spinnaker. Here is the terrform module to help you transform your aws account(https://registry.terraform.io/modules/tf-mod/spinnaker-managed-role/aws/). Back to this example when you've applied `spinnaker-managed-role` module to your target aws account you must add the ARN which is generated by the `spinnaker-managed-role` template into `assume_role_arn` variable.

```
module "spinnaker" {
  source  = "tf-mod/spinnaker/aws"
  version = "1.0.0"

  name                 = "spinnaker"
  stack                = "preprod"
  ...

  assume_role_arn      = ["arn:aws:iam::1234567890321:role/spinnaker-managed-your-env"]
}
```

### Generate minified kubernetes config
This terraform module will automatically generate shell script to make node-pool to join kubernetes (eks) cluster and install helm and tiller on it. You will find the `helm.sh` script under the directory has same name of eks cluster. Onto this directory and run that script to get the minified kubernetes configuration file. And it also will doing create a key-value configuration in configMap to allow node-pools to join cluster. When you get minified config file successfully you will have permission to control or manage the whole resources in `spinnaker` namespace.

[Important] Before you run this script you must configure your local environment to have proper permission to make a change on target aws account whatever you are using aws cli or aws-vault. Or it might be `default` when you didn't fill in anything. In this case you have to prepare aws credentials as default profile in `~/.aws/credentials` file.

`$ bash helm.sh`


### Create loadbalancers
You can create loadbalancers for public access of spinnaker. There will be helper script to create 2 or 3 loadbalancers using kuberntes command. You will find the `lb.sh`on same path of `helm.sh` script. After you apply this script you will see the endpoints of new loadbalancers. These urls will be used in next step. Please keep it in mind.

`$ bash lb.sh`

### Add dns records
Currently generating route 53 records from kubernetes is not stable. And this alpha feature needs full permission of route 53 of aws account but We didn't want to give admin policy to specific eks cluster. It could be a security whole to remove or change the route 53 resources are not ownd by that cluster. So this module suggests to utilise the terraform for managing route 53 records of loadbalancers which are generated by kubernetes in previous step. Here is an example to create dns records of loadbalancers. You can fill out `records` variable with endpoint from `lb.sh`.

```
# *.example.com

resource "aws_route53_record" "spin-ui" {
  count   = "${var.dns_zone_id == "" ? 0: 1}"
  zone_id = "${var.dns_zone_id}"
  name    = "spinnaker.${var.dns_zone}"
  type    = "CNAME"
  ttl     = 300
  records = ["2837423728147463.elb.us-east-1.amazonaws.com"]
}

resource "aws_route53_record" "spin-api" {
  count   = "${var.dns_zone_id == "" ? 0: 1}"
  zone_id = "${var.dns_zone_id}"
  name    = "spinnaker-api.${var.dns_zone}"
  type    = "CNAME"
  ttl     = 300
  records = ["8837463728347463.elb.us-east-1.amazonaws.com"]
}

resource "aws_route53_record" "spin-cli" {
  count   = "${var.dns_zone_id == "" ? 0: 1}"
  zone_id = "${var.dns_zone_id}"
  name    = "spinnaker-cli.${var.dns_zone}"
  type    = "CNAME"
  ttl     = 300
  records = ["2037471723346263.elb.us-east-1.amazonaws.com"]
}
```

### Register and Verify certificates
If you are using certificates manager service from aws, you should register your certificates and confirm that you are the owner of these certification by email or dns. This example shows how to valiate the certificates.

```
# certificates of *.example.com
# issued and managed by amazon certificate manager

resource "aws_acm_certificate" "cert" {
  count             = "${var.public_dns_zone == "" ? 0 : 1}"
  domain_name       = "*.${var.public_dns_zone}"
  validation_method = "DNS"

  tags = "${merge(
    map("Name", "${var.public_dns_zone}"),
    map("env", "preprod"),
  )}"

  lifecycle {
    create_before_destroy = true
  }
}

# cert. validation

locals {
  verify_name  = "${lookup(aws_acm_certificate.cert.domain_validation_options[0], "resource_record_name")}"
  verify_value = "${lookup(aws_acm_certificate.cert.domain_validation_options[0], "resource_record_value")}"
}

resource "aws_route53_record" "cert-validation" {
  zone_id = "${var.public_dns_zone_id}"
  name    = "${local.verify_name}"
  type    = "CNAME"
  ttl     = 300
  records = ["${local.verify_value}"]
}
```
